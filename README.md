# Kaggle-competition--Titanic-Learning-from-Disaster
The titanic problem in Kaggle is touted to be a begineers competition for ML enthusiasts. I was given 2 datasets, for training and testing. Train dataset is for tuning your ML model to generalize well when its deployed in test dataset. I've implemented Pandas libraries basic functionalities for feature engineering. Certain features got droped as they seem to be irrelevant. Similarly created feature mappings to reduce the dimensionality of input dataset and to scale the values along a narrow range. This is implemented over both the test and train dataset. Hence after complete pre processing the data we perform cross validation using Kfolds method to make our model able to generalize well. Checked the performance of Support Vector Machines and Gaussian Naive Bayes Classifier.Support Vector Machines was opted as it gave better accuracy. At the end created a dataset where the predictions for each and every passenger is made and appended to it.
Secured a Rank of 4878/22955 
